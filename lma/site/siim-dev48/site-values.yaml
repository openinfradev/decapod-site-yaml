apiVersion: openinfradev.github.com/v1
kind: HelmValuesTransformer
metadata:
  name: site

global:
  nodeSelector:
    taco-lma: enabled
  clusterName: siim-dev
  storageClassName: local-path
  serviceScrapeInterval: 10s
  federateScrapeInterval: 10s
  defaultPassword: password
  defaultUser: taco
charts:
- name: prometheus-operator
  override:
    prometheusOperator.nodeSelector: $(nodeSelector)

- name: eck-operator  

- name: prometheus  
  override:
    kubeEtcd.endpoints:
    - 192.168.48.202
    - 192.168.48.203
    - 192.168.48.204
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName: $(storageClassName)
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage: 200Gi
    prometheus.prometheusSpec.retention: 30d
    prometheus.prometheusSpec.externalLabels.taco_cluster: $(clusterName)
    prometheus.prometheusSpec.nodeSelector: $(nodeSelector)
    
- name: prometheus-fed-master    
  override:
    alertmanager.alertmanagerSpec.nodeSelector: $(nodeSelector)
    alertmanager.alertmanagerSpec.retention: 120h
    alertmanager.config.global.slack_api_url: https://hooks.slack.com/services/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
    prometheus.prometheusSpec.nodeSelector: $(nodeSelector)
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName: $(storageClassName)
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage: 500Gi
    alertmanager.config.receivers:
    - name: 'default-alert'
      slack_configs:
      - channel: "#alert"
        username: "Prometheus"
        send_resolved: true
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .  Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .    Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}
        text: |-
          {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message  }}{{ end }}
          {{ else }}
          {{ if gt (len .Alerts.Firing) 0 }}
          *Alerts Firing:*
            {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
          {{ end }}{{ end }}
          {{ if gt (len .Alerts.Resolved) 0 }}
          *Alerts Resolved:*
            {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
          {{ end }}{{ end }}
          {{ end }}
    - name: 'slack-alert'
      slack_configs:
      - channel: "#info-by-system"  #FIXME
        username: "Prometheus"
        send_resolved: true
        title: |-
          [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if or (and (eq (len .  Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .    Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}
        text: |-
          {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message  }}{{ end }}
          {{ else }}
          {{ if gt (len .Alerts.Firing) 0 }}
          *Alerts Firing:*
            {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
          {{ end }}{{ end }}
          {{ if gt (len .Alerts.Resolved) 0 }}
          *Alerts Resolved:*
            {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
          {{ end }}{{ end }}
          {{ end }}
    - name: 'telegram-alert'
      webhook_configs:
      - send_resolved: True
        url: http://prometheus-bot:9087/alert/-GROUP_ID
  
- name: prometheus-node-exporter  

- name: kube-state-metrics  
  override:
    nodeSelector: $(nodeSelector)

- name: prometheus-pushgateway  
  override:
    nodeSelector: $(nodeSelector)

- name: prometheus-process-exporter  
  override:
    conf.processes: dockerd,kubelet,kube-proxy,ntpd,node

- name: eck-resource  
  override:
    customResource.kibana.nodeSelector: $(nodeSelector)
    
    customResource.elasticsearch.nodeSets.master.nodeSelector: $(nodeSelector)
    customResource.elasticsearch.nodeSets.master.javaOpts: "-Xms2g -Xmx2g"
    customResource.elasticsearch.nodeSets.master.limitCpu: 2
    customResource.elasticsearch.nodeSets.master.limitMem: 4Gi
    customResource.elasticsearch.nodeSets.master.pvc.storageClassName: $(storageClassName)
    customResource.elasticsearch.nodeSets.master.pvc.size: 2Gi

    customResource.elasticsearch.nodeSets.hotdata.nodeSelector: $(nodeSelector)
    customResource.elasticsearch.nodeSets.hotdata.javaOpts: "-Xms10g -Xmx10g"
    customResource.elasticsearch.nodeSets.hotdata.limitCpu: 2
    customResource.elasticsearch.nodeSets.hotdata.limitMem: 20Gi
    customResource.elasticsearch.nodeSets.hotdata.pvc.storageClassName: $(storageClassName)
    customResource.elasticsearch.nodeSets.hotdata.pvc.size: 200Gi

    customResource.elasticsearch.nodeSets.client.enabled: true
    customResource.elasticsearch.nodeSets.client.nodeSelector: $(nodeSelector)
    customResource.elasticsearch.nodeSets.client.javaOpts: "-Xms10g -Xmx10g"
    customResource.elasticsearch.nodeSets.client.limitCpu: 2
    customResource.elasticsearch.nodeSets.client.limitMem: 20Gi
    customResource.elasticsearch.nodeSets.client.pvc.storageClassName: $(storageClassName)


- name: grafana  
  override:
    adminPassword: password
    persistence.storageClassName: $(storageClassName)

- name: fluentbit-operator  
  override:
    global.base_cluster_url: $(clusterName)
    fluentbitOperator.nodeSelector: $(nodeSelector)
    logExporter.nodeSelector: $(nodeSelector)

- name: fluentbit  
  override:
    global.base_cluster_url: $(clusterName)
    global.nodeSelector: $(nodeSelector)
    fluentbit.clusterName: $(clusterName)
    fluentbit.outputs.es.host: eck-elasticsearch-es-http.lma.svc.$(clusterName)
    fluentbit.outputs.kafka:
      enabled: false
      broker: "suy-prd-elk-kfk-01:9092,suy-prd-elk-kfk-02:9092,suy-prd-elk-kfk-03:9092,suy-prd-elk-kfk-04:9092,suy-prd-elk-kfk-05:9092,suy-prd-elk-kfk-06:9092,suy-prd-elk-kfk-07:9092,suy-prd-elk-kfk-08:9092,suy-prd-elk-kfk-09:9092,suy-prd-elk-kfk-10:9092,suy-prd-elk-kfk-11:9092"
      topic: taco-suy-adm-logs
    fluentbit.esTemplate.url: https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200
    fluentbit.nodeSelector: $(nodeSelector)

- name: addons  
  override:
    serviceMonitor.trident:
      enabled: false
      interval: $(serviceScrapeInterval)
    serviceMonitor.kubelet.interval: 30s

- name: fed-addons  
  override:
    metricbeat.enabled: false
    metricbeat.elasticsearch.host: https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200
    metricbeat.kibana.host: eck-kibana-dashboard-kb-http.lma.svc.$(clusterName):5601
    metricbeat.prometheus.hosts:
    - fed-master-prometheus.fed.svc.$(clusterName):9090
    tacoWatcher.host: taco-watcher.fed.svc.$(clusterName)
    tacoWatcher.joinCluster.body.kibanaUrl: http://eck-kibana-dashboard-kb-http.lma.svc.$(clusterName):5601
    tacoWatcher.joinCluster.body.grafanaUrl: http://grafana.fed.svc.$(clusterName)
    tacoWatcher.joinCluster.body.k8sUrl: https://kubernetes.default.svc.$(clusterName)
    kibanaInit.url: http://eck-kibana-dashboard-kb-http.lma.svc.$(clusterName):5601
    serviceMonitor.grafana.interval: $(serviceScrapeInterval)
    serviceMonitor.federations:
    - name: admin-federate
      interval: $(federateScrapeInterval)
      namespace: lma
      selector:
        app: prometheus
        prometheus: lma-prometheus
      port: 9090
    # - name: siim-main-federate
    #   interval: $(federateScrapeInterval)
    #   addresses:
    #   - 192.168.48.202
    #   - 192.168.5.61
    #   port: 30008

- name: prometheus-adapter  
  override:
    nodeSelector: $(nodeSelector)

- name: kubernetes-event-exporter  
  override:
    conf.default.hosts:
    - "https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200"

- name: thanos
  override:
    global.storageClass: $(storageClassName)
    clusterDomain: $(clusterName)
    objstoreConfig: |-
      type: s3
      config:
        bucket: thanos
        endpoint: {{ include "thanos.minio.fullname" . }}.fed.svc.$(clusterName):9000
        access_key: $(defaultUser)
        secret_key: $(defaultPassword)
        insecure: true
    bucketweb.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    storegateway.nodeSelector: $(nodeSelector)
    ruler.nodeSelector: $(nodeSelector)      
    compactor.persistence.size: 8Gi
    storegateway.persistence.size: 80Gi
    querier:
      stores:
        - 192.168.5.61:30901
        - 192.168.48.202:30901
    ruler.alertmanagers:
      - http://prometheus-operator-alertmanager:9093
    ruler.config: |-
        groups:
          - name: "metamonitoring"
            rules:
              - alert: "PrometheusDown"
                expr: absent(up{prometheus="monitoring/prometheus-operator"})
    minio.accessKey.password: $(defaultUser)
    minio.secretKey.password: $(defaultPassword)
    minio.defaultBuckets: thanos
    minio.persistence.storageClass: $(storageClassName)
    minio.persistence.accessMode: ReadWriteOnce
    minio.persistence.size: 100Gi