apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: addons
  name: addons
spec:
  chart:
    name: lma-addons
    repository: https://openinfradev.github.io/hanu-helm-repo
    version: 1.0.7
  helmVersion: v3
  releaseName: addons
  targetNamespace: lma
  values:
    grafanaDashboard:
      enabled: false
    metricbeat:
      enabled: false
      loglevel: error
    prometheusRules:
      aggregation:
        enabled: true
      alert:
        enabled: false
    serviceMonitor:
      calico:
        enabled: true
        interval: 10s
      ceph:
        enabled: false
        interval: 10s
        mon_hosts: []
      grafana:
        enabled: false
      kubeStateMetrics:
        interval: 10s
      kubelet:
        enabled: true
        interval: 30s
      nodeExporter:
        interval: 10s
      processExporter:
        enabled: true
        interval: 10s
        selector:
          matchLabels:
            application: process_exporter
            component: metrics
            release_group: prometheus-process-exporter
      trident:
        enabled: true
        interval: 10s
    tacoWatcher:
      rbac:
        create: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-operator
  name: eck-operator
spec:
  chart:
    name: eck-operator
    repository: https://helm.elastic.co
    version: 1.3.1
  helmVersion: v3
  releaseName: eck-operator
  targetNamespace: elastic-system
  values:
    config:
      containerRegistry: docker.elastic.co
    installCRDs: true
    replicaCount: 1
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: eck-resource
  name: eck-resource
spec:
  chart:
    name: eck-resource
    repository: https://openinfradev.github.io/hanu-helm-repo
    version: 1.0.0
  helmVersion: v3
  releaseName: eck-resource
  targetNamespace: lma
  values:
    customResource:
      elasticsearch:
        nodeSets:
          client:
            enabled: true
            javaOpts: -Xms10g -Xmx10g
            limitCpu: 2
            limitMem: 20Gi
            nodeSelector:
              taco-lma: enabled
            pvc:
              storageClassName: rbd
          hotdata:
            javaOpts: -Xms10g -Xmx10g
            limitCpu: 2
            limitMem: 20Gi
            nodeSelector:
              taco-lma: enabled
            pvc:
              size: 200Gi
              storageClassName: rbd
          master:
            javaOpts: -Xms2g -Xmx2g
            limitCpu: 2
            limitMem: 4Gi
            nodeSelector:
              taco-lma: enabled
            pvc:
              size: 2Gi
              storageClassName: rbd
      kibana:
        nodeSelector:
          taco-lma: enabled
    elasticsearch:
      adminPassword: tacoword
      count: 3
      enabled: true
      nodeSets:
        client:
          count: 1
          enabled: true
          javaOpts: -Xms2g -Xmx2g
          limitCpu: 2
          limitMem: 4Gi
          nodeSelector:
            taco-lma: enabled
          pvc:
            size: 0.5Gi
            storageClassName: ceph
        hotdata:
          count: 3
          enabled: true
          javaOpts: -Xms2g -Xmx2g
          limitCpu: 2
          limitMem: 4Gi
          nodeSelector:
            taco-lma: enabled
          pvc:
            size: 100Gi
            storageClassName: ceph
        master:
          count: 3
          enabled: true
          javaOpts: -Xms2g -Xmx2g
          limitCpu: 2
          limitMem: 4Gi
          nodeSelector:
            taco-lma: enabled
          pvc:
            size: 2Gi
            storageClassName: ceph
        warmdata:
          count: 2
          enabled: false
          javaOpts: -Xms2g -Xmx2g
          limitCpu: 1
          limitMem: 2Gi
          nodeSelector:
            taco-lma: enabled
          pvc:
            size: 200Gi
            storageClassName: ceph
      version: 7.5.1
    kibana:
      enabled: true
      http:
        service:
          spec:
            ports:
            - name: http
              nodePort: 30001
              port: 5601
              targetPort: 5601
            type: NodePort
        tls:
          selfSignedCertificate:
            disabled: true
      limitCpu: 4
      limitMem: 8Gi
      nodeSelector:
        taco-lma: enabled
      version: 7.5.1
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fed-addons
  name: fed-addons
spec:
  chart:
    name: lma-addons
    repository: https://openinfradev.github.io/hanu-helm-repo
    version: 1.0.7
  helmVersion: v3
  releaseName: fed-addons
  targetNamespace: fed
  values:
    grafanaDashboard:
      enabled: true
      sidecar:
        datasources:
          prometheusAddress: fed-master-prometheus:9090
    kibanaInit:
      enabled: true
      url: http://taco-kibana-dashboard-kb-http.lma.svc.siim-dev:5601
    metricbeat:
      addtionalModules: []
      elasticsearch:
        host: https://taco-elasticsearch-es-http.lma.svc.siim-dev:9200
        password: tacoword
        username: elastic
      enabled: false
      kibana:
        host: taco-kibana-dashboard-kb-http.lma.svc.siim-dev:5601
      prometheus:
        hosts:
        - fed-master-prometheus.fed.svc.siim-dev:9090
    prometheusRules:
      aggregation:
        enabled: false
      alert:
        enabled: true
    serviceMonitor:
      calico:
        enabled: false
      ceph:
        enabled: false
      federations:
      - interval: 10s
        name: admin-federate
        namespace: lma
        port: 9090
        selector:
          app: prometheus
          prometheus: lma-prometheus
      - addresses:
        - 192.168.105.161
        - 192.168.105.162
        - 192.168.105.163
        interval: 10s
        name: siim-main-federate
        port: 30008
      grafana:
        enabled: true
        interval: 10s
      kubeStateMetrics:
        enabled: false
      nodeExporter:
        enabled: false
      processExporter:
        enabled: false
    tacoWatcher:
      host: taco-watcher.fed.svc.siim-dev
      joinCluster:
        body:
          grafanaUrl: http://grafana.fed.svc.siim-dev
          isMain: true
          k8sUrl: https://kubernetes.default.svc.siim-dev
          kibanaUrl: http://taco-kibana-dashboard-kb-http.lma.svc.siim-dev:5601
          menu:
            enabled: true
        enabled: false
      port: 32000
      rbac:
        create: true
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluentbit
  name: fluentbit
spec:
  chart:
    name: fluentbit-operator
    repository: https://openinfradev.github.io/hanu-helm-repo
    skipDepUpdate: true
    version: 1.0.9
  helmVersion: v3
  releaseName: fluentbit
  targetNamespace: lma
  values:
    fluentbit:
      alerts:
        makeAlertRule: true
      clusterName: siim-dev
      daemonset:
        spec:
          pod:
            tolerations:
            - key: node-role.kubernetes.io/master
              operator: Exists
            - key: node-role.kubernetes.io/node
              operator: Exists
      enabled: true
      esTemplate:
        enabled: true
        ilms:
        - json:
            policy:
              phases:
                delete:
                  actions:
                    delete: {}
                  min_age: 14d
                hot:
                  actions:
                    rollover:
                      max_age: 1d
                      max_docs: 5000000000
                      max_size: 30gb
                    set_priority:
                      priority: 100
          name: hot-delete-14days
        - json:
            policy:
              phases:
                delete:
                  actions:
                    delete: {}
                  min_age: 7d
                hot:
                  actions:
                    rollover:
                      max_age: 1d
                      max_docs: 5000000000
                      max_size: 30gb
                    set_priority:
                      priority: 100
          name: hot-delete-7days
        - json:
            policy:
              phases:
                delete:
                  actions:
                    delete: {}
                  min_age: 3h
                hot:
                  actions:
                    rollover:
                      max_age: 1h
                      max_docs: 5000000000
                      max_size: 30gb
                    set_priority:
                      priority: 100
          name: hot-delete-3hour
        password: tacoword
        templates:
        - json:
            index_patterns: platform*
            settings:
              index.lifecycle.name: hot-delete-14days
              index.lifecycle.rollover_alias: platform
              number_of_replicas: 1
              number_of_shards: 3
              refresh_interval: 30s
          name: platform
        - json:
            index_patterns: container*
            settings:
              index.lifecycle.name: hot-delete-3hour
              index.lifecycle.rollover_alias: container
              number_of_replicas: 1
              number_of_shards: 3
              refresh_interval: 30s
          name: application
        - json:
            index_patterns: syslog*
            settings:
              index.lifecycle.name: hot-delete-14days
              index.lifecycle.rollover_alias: syslog
              number_of_replicas: 1
              number_of_shards: 2
              refresh_interval: 30s
          name: syslog
        url: https://taco-elasticsearch-es-http.lma.svc.siim-dev:9200
        username: elastic
      exclude:
      - key: $kubernetes['container_name']
        value: kibana|elasticsearch|fluent-bit
      nodeSelector:
        taco-lma: enabled
      outputs:
        es:
          enabled: true
          host: taco-elasticsearch-es-http.lma.svc.siim-dev
          password: dGFjb3dvcmQ=
          port: 9200
          username: ZWxhc3RpYw==
        http:
          enabled: true
        kafka:
          broker: suy-prd-elk-kfk-01:9092,suy-prd-elk-kfk-02:9092,suy-prd-elk-kfk-03:9092,suy-prd-elk-kfk-04:9092,suy-prd-elk-kfk-05:9092,suy-prd-elk-kfk-06:9092,suy-prd-elk-kfk-07:9092,suy-prd-elk-kfk-08:9092,suy-prd-elk-kfk-09:9092,suy-prd-elk-kfk-10:9092,suy-prd-elk-kfk-11:9092
          enabled: false
          topic: taco-suy-adm-logs
      targetLogs:
      - bufferChunkSize: 2M
        bufferMaxSize: 5M
        index: container
        memBufLimit: 20MB
        multi_index:
        - index: platform
          key: $kubernetes['namespace_name']
          value: kube-system|lma|fed|argo|openstack
        name: dockerlog
        parser: docker
        path: /var/log/containers/*.log
        tag: kube.*
        type: fluent
      - index: syslog
        name: syslog
        parser: syslog-rfc5424
        path: /var/log/messages
        tag: syslog.*
        type: syslog
    fluentbitOperator:
      cleanupCustomResource: false
      createCustomResource: false
      enabled: false
    global:
      base_cluster_url: siim-dev
      nodeSelector:
        taco-lma: enabled
    image:
      hyperkube:
        tag: v1.17.6
    logExporter:
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: fluentbit-operator
  name: fluentbit-operator
spec:
  chart:
    name: fluentbit-operator
    repository: https://openinfradev.github.io/hanu-helm-repo
    skipDepUpdate: true
    version: 1.0.9
  helmVersion: v3
  releaseName: fluentbit-operator
  targetNamespace: lma
  values:
    fluentbit:
      enabled: false
    fluentbitOperator:
      cleanupCustomResource: true
      createCustomResource: true
      enabled: true
      nodeSelector:
        taco-lma: enabled
      resources:
        limits:
          cpu: "2"
          memory: 200Mi
        requests:
          cpu: 100m
          memory: 20Mi
    global:
      base_cluster_url: siim-dev
    image:
      hyperkube:
        tag: v1.17.6
    logExporter:
      enabled: true
      nodeSelector:
        taco-lma: enabled
      serviceMonitor:
        enabled: false
        interval: 1m
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: grafana
  name: grafana
spec:
  chart:
    name: grafana
    repository: https://grafana.github.io/helm-charts
    version: 5.5.7
  helmVersion: v3
  releaseName: grafana
  targetNamespace: fed
  values:
    adminPassword: password
    grafana.ini:
      plugins:
        devopsprodigy-kubegraf-app: true
        grafana-piechart-panel: true
        vonage-status-panel: true
    persistence:
      enabled: true
      size: 10G
      storageClassName: rbd
    plugins:
    - vonage-status-panel
    - grafana-piechart-panel
    - devopsprodigy-kubegraf-app
    service:
      nodePort: 30009
      type: NodePort
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
      datasources:
        enabled: true
        label: grafana_datasource
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kube-state-metrics
  name: kube-state-metrics
spec:
  chart:
    name: kube-state-metrics
    repository: https://charts.bitnami.com/bitnami
    version: 1.1.3
  helmVersion: v3
  releaseName: kube-state-metrics
  targetNamespace: lma
  values:
    collectors:
      verticalpodautoscalers: false
    nodeSelector:
      taco-lma: enabled
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: kubernetes-event-exporter
  name: kubernetes-event-exporter
spec:
  chart:
    name: kubernetes-event-exporter
    repository: https://openinfradev.github.io/hanu-helm-repo
    version: 1.0.0
  helmVersion: v3
  releaseName: kubernetes-event-exporter
  targetNamespace: lma
  values:
    conf:
      additionalReceivers: {}
      additionalRoutes: {}
      default:
        additionalDefaultReceivers: []
        hosts:
        - https://taco-elasticsearch-es-http.lma.svc.siim-dev:9200
        index: kube-events
        password: tacoword
        user: elastic
      logFormat: json
      logLevel: error
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus
  name: prometheus
spec:
  chart:
    name: kube-prometheus-stack
    repository: https://prometheus-community.github.io/helm-charts
    version: 13.7.2
  helmVersion: v3
  releaseName: prometheus
  targetNamespace: lma
  values:
    alertmanager:
      enabled: false
    coreDns:
      enabled: true
      serviceMonitor:
        interval: 10s
    defaultRules:
      create: false
    fullnameOverride: lma
    global:
      rbac:
        create: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: true
      serviceMonitor:
        interval: 10s
    kubeControllerManager:
      enabled: true
      serviceMonitor:
        interval: 10s
    kubeDns:
      enabled: false
      serviceMonitor:
        interval: 10s
    kubeEtcd:
      enabled: true
      endpoints:
      - 192.168.105.161
      - 192.168.105.162
      - 192.168.105.163
      serviceMonitor:
        caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
        certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
        insecureSkipVerify: false
        interval: 10s
        keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
        scheme: https
        serverName: localhost
    kubeProxy:
      enabled: true
      serviceMonitor:
        interval: 10s
    kubeScheduler:
      enabled: true
      serviceMonitor:
        interval: 10s
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    prometheus:
      prometheusSpec:
        externalLabels:
          taco_cluster: siim-dev
        nodeSelector:
          taco-lma: enabled
        replicas: 1
        retention: 30d
        ruleNamespaceSelector:
          matchLabels:
            name: lma
        ruleSelectorNilUsesHelmValues: false
        secrets:
        - etcd-client-cert
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: lma
        serviceMonitorSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 200Gi
              storageClassName: rbd
        thanos:
          version: v0.11.0
      service:
        nodePort: 30008
        type: NodePort
      thanosService:
        annotations: {}
        enabled: false
        labels: {}
        nodePort: 30901
        port: 10901
        portName: grpc
        targetPort: grpc
        type: NodePort
    prometheusOperator:
      admissionWebhooks:
        enabled: false
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-adapter
  name: prometheus-adapter
spec:
  chart:
    name: prometheus-adapter
    repository: https://prometheus-community.github.io/helm-charts
    version: 2.5.1
  helmVersion: v3
  releaseName: prometheus-adapter
  targetNamespace: lma
  values:
    nodeSelector:
      taco-lma: enabled
    prometheus:
      port: 9090
      url: http://lma-prometheus
    rules:
      default: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-fed-master
  name: prometheus-fed-master
spec:
  chart:
    name: kube-prometheus-stack
    repository: https://prometheus-community.github.io/helm-charts
    version: 13.7.2
  helmVersion: v3
  releaseName: prometheus-fed-master
  targetNamespace: fed
  values:
    alertmanager:
      alertmanagerSpec:
        nodeSelector:
          taco-lma: enabled
        retention: 120h
      config:
        global:
          slack_api_url: https://hooks.slack.com/services/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
          smtp_auth_password: null
          smtp_auth_username: null
          smtp_from: null
          smtp_smarthost: null
        receivers:
        - name: default-alert
          slack_configs:
          - channel: '#alert'
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message  }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing
              | len }}{{ end }}] {{ if or (and (eq (len .  Alerts.Firing) 1) (eq (len
              .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved)
              1)) }} {{ range .    Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{
              range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: slack-alert
          slack_configs:
          - channel: '#info-by-system'
            send_resolved: true
            text: |-
              {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message  }}{{ end }}
              {{ else }}
              {{ if gt (len .Alerts.Firing) 0 }}
              *Alerts Firing:*
                {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ if gt (len .Alerts.Resolved) 0 }}
              *Alerts Resolved:*
                {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
              {{ end }}{{ end }}
              {{ end }}
            title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing
              | len }}{{ end }}] {{ if or (and (eq (len .  Alerts.Firing) 1) (eq (len
              .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved)
              1)) }} {{ range .    Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{
              range .Alerts.Resolved }}{{ .Labels.alertname }}{{ end }}{{ end }}'
            username: Prometheus
        - name: telegram-alert
          webhook_configs:
          - send_resolved: true
            url: http://prometheus-bot:9087/alert/-GROUP_ID
        route:
          group_by:
          - alertname
          group_wait: 10s
          receiver: default-alert
          repeat_interval: 1h
          routes:
          - group_by:
            - alertname
            match:
              severity: page
            receiver: slack-alert
      enabled: true
    coreDns:
      enabled: false
    defaultRules:
      create: false
    fullnameOverride: fed-master
    global:
      rbac:
        create: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    prometheus:
      prometheusSpec:
        externalLabels:
          taco_cluster: federation-master
        nodeSelector:
          taco-lma: enabled
        replicas: 1
        ruleNamespaceSelector:
          matchLabels:
            name: fed
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorNamespaceSelector:
          matchLabels:
            name: fed
        serviceMonitorSelectorNilUsesHelmValues: false
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 500Gi
              storageClassName: rbd
      service:
        nodePort: 30018
        type: NodePort
    prometheusOperator:
      admissionWebhooks:
        enabled: false
      enabled: false
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-node-exporter
  name: prometheus-node-exporter
spec:
  chart:
    name: prometheus-node-exporter
    repository: https://prometheus-community.github.io/helm-charts
    version: 1.11.2
  helmVersion: v3
  releaseName: prometheus-node-exporter
  targetNamespace: lma
  values:
    extraArgs:
    - --no-collector.hwmon
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-operator
  name: prometheus-operator
spec:
  chart:
    name: kube-prometheus-stack
    repository: https://prometheus-community.github.io/helm-charts
    version: 13.7.2
  helmVersion: v3
  releaseName: prometheus-operator
  targetNamespace: lma
  values:
    alertmanager:
      enabled: false
    coreDns:
      enabled: false
    defaultRules:
      create: false
    global:
      rbac:
        create: false
    grafana:
      enabled: false
    kubeApiServer:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeDns:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeStateMetrics:
      enabled: false
    kubelet:
      enabled: false
    nodeExporter:
      enabled: false
    prometheus:
      enabled: false
    prometheusOperator:
      cleanupCustomResource: true
      cleanupCustomResourceBeforeInstall: true
      createCustomResource: true
      enabled: true
      nodeSelector:
        taco-lma: enabled
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-process-exporter
  name: prometheus-process-exporter
spec:
  chart:
    name: prometheus-process-exporter
    repository: https://openinfradev.github.io/hanu-helm-repo
    skipDepUpdate: true
    version: 0.1.0
  helmVersion: v3
  releaseName: prometheus-process-exporter
  targetNamespace: lma
  values:
    conf:
      processes: dockerd,kubelet,kube-proxy,ntpd,node
    images:
      pull_policy: Always
    labels:
      process_exporter:
        process_selector_key: process-exporter
        process_selector_value: enabled
    pod:
      lifecycle:
        upgrades:
          daemonsets:
            process_exporter:
              max_unavailable: 30%
      mandatory_access_control:
        type: null
      tolerations:
        process_exporter:
          enabled: true
          tolerations:
          - key: node-role.kubernetes.io/master
            operator: Exists
          - key: node-role.kubernetes.io/node
            operator: Exists
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: prometheus-pushgateway
  name: prometheus-pushgateway
spec:
  chart:
    name: prometheus-pushgateway
    repository: https://prometheus-community.github.io/helm-charts
    version: 1.5.1
  helmVersion: v3
  releaseName: prometheus-pushgateway
  targetNamespace: lma
  values:
    nodeSelector:
      taco-lma: enabled
    service:
      nodePort: 30010
      type: NodePort
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: taco-watcher
  name: taco-watcher
spec:
  chart:
    name: taco-watcher
    repository: https://openinfradev.github.io/hanu-helm-repo
    version: 0.1.0
  helmVersion: v3
  releaseName: taco-watcher
  targetNamespace: fed
  values:
    config:
      grafana:
        authkey: admin:password
      initDB: true
      kibana:
        authkey: elastic:tacoword
      password: password
      username: taco
    nodeSelector: {}
    resources:
      storageClassName: TO_BE_FIXED
    service:
      nodePort: 32000
      port: 32000
      proxy_from: 32001
      proxy_to: 32009
      targetPort: 32000
      type: NodePort
  wait: true
---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  labels:
    name: thanos
  name: thanos
spec:
  chart:
    name: thanos
    repository: https://charts.bitnami.com/bitnami
    version: 3.4.0
  helmVersion: v3
  releaseName: thanos
  targetNamespace: fed
  values:
    bucketweb:
      enabled: true
      logLevel: info
      nodeSelector:
        taco-lma: enabled
      refresh: 30m
      service:
        http:
          port: 8080
        type: ClusterIP
      timeout: 5m
    clusterDomain: cluster.local
    compactor:
      consistencyDelay: 30m
      enabled: true
      logLevel: info
      nodeSelector:
        taco-lma: enabled
      persistence:
        accessModes:
        - ReadWriteOnce
        enabled: true
        size: 8Gi
      retentionResolution1h: 10y
      retentionResolution5m: 30d
      retentionResolutionRaw: 30d
      service:
        http:
          port: 9090
        type: ClusterIP
    global:
      storageClass: ceph
    metrics:
      enabled: false
      serviceMonitor:
        enabled: false
    minio:
      accessKey:
        password: taco
      defaultBuckets: thanos
      enabled: true
      secretKey:
        password: password
      service:
        nodePort: 30002
        type: NodePort
    objstoreConfig: |-
      type: s3
      config:
        bucket: thanos
        endpoint: {{ include "thanos.minio.fullname" . }}.fed.svc.cluster.local:9000
        access_key: taco
        secret_key: password
        insecure: true
    querier:
      stores: TO_BE_FIXED
    query:
      nodeSelector:
        taco-lma: enabled
    queryFrontend:
      nodeSelector:
        taco-lma: enabled
      service:
        http:
          nodePort: 30007
          port: 9090
        type: NodePort
    ruler:
      alertmanagers:
      - http://prometheus-operator-alertmanager:9093
      config: |-
        groups:
          - name: "metamonitoring"
            rules:
              - alert: "PrometheusDown"
                expr: absent(up{prometheus="monitoring/prometheus-operator"})
      enabled: true
      evalInterval: 1m
      logLevel: info
      nodeSelector:
        taco-lma: enabled
      persistence:
        accessModes:
        - ReadWriteOnce
        enabled: true
        size: 8Gi
      podManagementPolicy: OrderedReady
      replicaCount: 1
      service:
        grpc:
          port: 10901
        http:
          port: 9090
        type: ClusterIP
      updateStrategyType: RollingUpdate
    storegateway:
      autoscaling:
        enabled: false
      enabled: true
      logLevel: info
      nodeSelector:
        taco-lma: enabled
      persistence:
        accessModes:
        - ReadWriteOnce
        enabled: true
        size: 8Gi
      service:
        grpc:
          port: 10901
        http:
          port: 9090
        type: ClusterIP
    volumePermissions:
      enabled: false
      image:
        registry: docker.io
        repository: bitnami/minideb
        tag: buster
  wait: true
